{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 2: Bayseian and frequentist schools of thought\n",
    "## 2.1: Bayes' Theorem\n",
    "\n",
    "Consider two events, $A$ and $B$.\n",
    "\n",
    "The joint probability of both $A$ and $B$ occuring, $P(A\\cap B)$, can be written as:\n",
    "\n",
    "\\begin{align}\n",
    "P(A\\cap B) = P(A|B) \\times P(B),\n",
    "\\end{align}\n",
    "\n",
    "where $P(A|B)$ represents the probability of $A$ occuring given that $B$ has occurred. But it can <i>also</i> be written in the reverse sense:\n",
    "\n",
    "\\begin{align}\n",
    "P(A\\cap B) = P(B|A) \\times P(A).\n",
    "\\end{align}\n",
    "\n",
    "Thus, these two representations are equal and we can write:\n",
    "\n",
    "\\begin{align}\n",
    "P(A|B) = \\frac{P(B|A)}{P(B)}P(A).\n",
    "\\end{align}\n",
    "\n",
    "$P(A)$ and $P(B)$ are *unconditional* probabilities. In the context of Bayes' theorem, they are our *prior probabilities* (also called Bayesian priors or simply priors). \n",
    "\n",
    "$P(B|A)/P(B)$ is the *support* $B$ has for $A$\n",
    "\n",
    "$P(A|B)$ is the *posterior*: the degree of belief after accounting for $B$.  \n",
    "\n",
    "Let's do a very simple example with a 6-sided die.  \n",
    "\n",
    "What is the probability of rolling a 4, given we rolled between 1 and 4, $P(4| 1-4)$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "....."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*example 1.5 (page 19) from Data Analysis in High Energy Physics*\n",
    "\n",
    "Now let's extend to a more complicated situation.\n",
    "\n",
    "In some city, 15% of taxis are yellow ($Y$) and 85% are green ($G$). A taxi was in an accident, and the eyewitness says that it was yellow. The police have established that eyewitness statements get the colour correct 80% of the time ($c$), and are wrong 20% of the time ($w$). \n",
    "\n",
    "What is the probability that the taxi really was yellow - $P(Y | \\mbox{witness says yellow})$?\n",
    "\n",
    "Let's write this out using Bayes' theorem. \n",
    "\n",
    "\\begin{align}\n",
    "P(Y | \\mbox{witness says yellow}) = \\frac{P( \\mbox{witness says yellow}|Y) \\times P(Y)}{P(\\mbox{witness says yellow})}\n",
    "\\end{align}\n",
    "\n",
    "But we don't really know what the denominator $P(\\mbox{witness says yellow})$ really is, in so many words. However, we can expand it using the *law of total probability*:\n",
    "\n",
    "\\begin{align}\n",
    "P(\\mbox{witness says yellow}) &= P(\\mbox{witness says yellow} | Y )P(Y) + P(\\mbox{witness says yellow} | G)P(G)\\\\\n",
    "&= P(c)P(Y) + P(w)P(G).\n",
    "\\end{align}\n",
    "\n",
    "Then, the full equation becomes:\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "P(Y | \\mbox{witness says yellow}) &= \\frac{P( \\mbox{witness says yellow} | Y) \\times P(Y)}{P(c)P(Y) + P(w)P(G)}\\\\\n",
    "&= \\frac{P(c)P(Y)}{P(c)P(Y) + P(w)P(G)}\\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plug in the numbers..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2: Applying Bayes' theorem to the case study from Lesson 1\n",
    "\n",
    "Recall that we calculated some likelihoods of our observation occuring under various scenarios ($L(N_{obs} | H_{s+b}),  L(N_{obs} | H_{b})$. However, we really wanted to know $P(\\mbox{signal+bkg hypothesis} | \\mbox{observation}) = P(H_{s+b} | N_{obs} = 9)$. \n",
    "\n",
    "Now that we have Bayes' theorem, we can figure this out:\n",
    "\n",
    "\\begin{align}\n",
    "P(H_{s+b} | N_{obs}) = \\frac{L(N_{obs} | H_{s+b}) \\times P(H_{s+b})}{P(N_{obs})}.\n",
    "\\end{align}\n",
    "\n",
    "It's not possible in our scenario to explicity know $P(N_{obs})$ - the unconditional probability of observing e.g. 9 events, but we can expand this with the law of total probability:\n",
    "\n",
    "\\begin{align}\n",
    "P(N_{obs}) = \\sum_{i} L(N_{obs} | H_{i}) \\times P(H_{i}),\n",
    "\\end{align}\n",
    "\n",
    "where the sum runs over *all* possible hypotheses. There are, of course, an infinite number of possible hypotheses - but we have prior belief that the probability of most of them occurring is vanishingly small. The only two hypotheses that we believe have any real possibility, under our test, are our background-only hypothesis and our signal+background hypothesis. Thus,\n",
    "\n",
    "\\begin{align}\n",
    "P(H_{s+b} | N_{obs}) = \\frac{L(N_{obs} | H_{s+b}) \\times P(H_{s+b})}{L(N_{obs}|H_{b})P(H_{b}) + L(N_{obs}|H_{s+b})P(H_{s+b})  }.\n",
    "\\end{align}\n",
    "\n",
    "$P(H_{b})$ and $P(H_{s+b})$ are our *priors*: what is our prior belief in $H_{b}$ and $H_{s+b}$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this context it is intructive to compare the formulation of evidence for discovery of a new particle in both frameworks. In the Bayesian framework evidence for a hypothesis is case as an odds ratio. The ratio of probabilities prior to the experiment defines the *prior odds ratio*:\n",
    "\n",
    "\\begin{align}\n",
    "O_{\\mbox{prior}}=\\frac{P(H_{s+b})}{P(H_{b})}=\\frac{P(H_{s+b})}{1−P(H_{s+b})}.\n",
    "\\end{align}\n",
    "\n",
    "The posterior odds ratio is defined as the ratio of posterior probabilities:\n",
    "\n",
    "\\begin{align}\n",
    "O_{\\mbox{posterior}}&=\\frac{P(H_{s+b}|N_{obs})}{P(H_{b} | N_{obs})}\\\\\n",
    " &= \\frac{L(N_{obs} | H_{s+b}) P(H_{s+b})}{L(N_{obs}(H_{b})P(H_{b})}\\\\\n",
    " &=L(N_{obs}|H_{s+b})L(N_{obs} | H_{b})⋅O_{\\mbox{prior}}.\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "The posterior odds ratio can be factorized as the prior odds ratio multiplied with the so-called *Bayes factor*, which contains the experimental information. \n",
    "\n",
    "For example, for equal prior odds (both prior probabilities = 0.5) and an observation $L(N_{obs}|H_{b})=0.036$ and $L(N_{obs}|H_{s+b})=0.125$, the posterior odds ratio becomes 3.5:1 in favor of the s+b hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3: The frequentist school of thought\n",
    "\n",
    "*Inference*: the process of getting from the data to the theory, e.g. our probability $P(H_{s+b} | N_{obs})$. In 2.2, we used Bayesian inference to calculate $P(H_{s+b} | N_{obs})$.\n",
    "\n",
    "Frequentist statements deal with *frequencies* of something occurring. In the frequentist framework no probabilities can be assigned to theories (i.e. no priors!) as there is no concept of repetition for hypotheses. A frequentist would say $P(H_{b})$ is either 0 or 1. It either is or it is not - it's not something we can test 1000 times and then present our estimated frequency of it occurring in the future.\n",
    "\n",
    "Framed another way, we can say that frequentist statements are restricted to probabilities on data, while in the Bayesian framework probabilities are assigned to constants of nature.\n",
    "\n",
    "Consider the current top quark mass measurement, $173.2 \\pm 0.9$ GeV. A Bayesian interpretation of this is that there is a 68% chance that the top mass lies within $[172.3, 174.1]$. But the frequentist interpretation says the top mass is just some value $m_{t}$, which is either in the $1\\sigma = 68\\%$ window or not. What the frequentist can say is that $172.3 < m_{t} < 174.1$ GeV has a 68% chance of being true, or we say $172.3 < m_{t} < 174.1$ GeV with 68% confidence ($172.3 < m_{t} < 174.1$ GeV is the $68\\%$ confidence interval).\n",
    "\n",
    "Generally, in particle physics, present results as *frequentist* statements, thus avoiding the discussion of appropriate priors.\n",
    "\n",
    "discuss: https://xkcd.com/1132/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
